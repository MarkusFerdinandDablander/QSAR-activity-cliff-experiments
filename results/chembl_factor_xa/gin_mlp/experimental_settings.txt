Experimental Settings: 
 
target_name = chembl_factor_xa
n_molecules = 3605
n_mmps = 16599
method_name = gin_mlp
 
k_splits = 2
m_reps = 3
random_state_cv = 42
 
optuna_options = {'h_iters': 20, 'frac_train': 0.8, 'data_splitting_seed': 42, 'performance_metric': <function mean_absolute_error at 0x7fe39dd6d710>, 'direction': 'minimize', 'sampler': <optuna.samplers._tpe.sampler.TPESampler object at 0x7fe299973690>, 'pruner': <optuna.pruners._nop.NopPruner object at 0x7fe2999737d0>}
 
mlp_hyperparameter_grid = {'architecture': [(None, None, 1), (None, None, None, None, None, None, 1), (None, None, None, None, None, None, None, None, None, None, None, 1)], 'hidden_activation': [ReLU()], 'output_activation': [Identity()], 'use_bias': [True], 'hidden_dropout_rate': [0, 0.25], 'hidden_batchnorm': [True]}
 
gin_hyperparameter_grid = {'n_conv_layers': [1, 2, 3], 'input_dim': [79], 'hidden_dim': [64, 128, 256], 'mlp_n_hidden_layers': [2], 'mlp_hidden_activation': [ReLU()], 'mlp_output_activation': [Identity()], 'mlp_use_bias': [True], 'mlp_hidden_dropout_rate': [0, 0.25], 'mlp_hidden_batchnorm': [True], 'eps': [0], 'train_eps': [False], 'pooling_operation': [<function global_max_pool at 0x7fe29f0b00e0>]}
 
train_hyperparameter_grid = {'batch_size': [32, 64, 128], 'dataloader_shuffle': [True], 'dataloader_drop_last': [True], 'learning_rate': [0.01, 0.001], 'lr_lambda': [<function <lambda> at 0x7fe299808050>, <function <lambda> at 0x7fe29983cc20>], 'weight_decay': [0.1, 0.01], 'num_epochs': [500], 'loss_function': [MSELoss()], 'optimiser': [<class 'torch.optim.adamw.AdamW'>], 'performance_metrics': ['regression'], 'print_results_per_epochs': [None]}
 
runtime = 05:38:05
 
